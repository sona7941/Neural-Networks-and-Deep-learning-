{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8f00c3c",
   "metadata": {},
   "source": [
    "## House Price Prediction with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3251741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4866197",
   "metadata": {},
   "source": [
    "## Synthetic Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "996a21cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 4) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Generate dataset\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "X = np.random.randint(1, 50, (1000, 4))  # bedrooms, sqft, age, bathrooms\n",
    "y = np.sum(X * [15000, 100, 1000, 8000], axis=1) + np.random.normal(0, 10000, 1000)\n",
    "print (X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb61eda4",
   "metadata": {},
   "source": [
    "## Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d64f454f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 1000 samples, 4 features\n",
      "Train: 800, Test: 200\n"
     ]
    }
   ],
   "source": [
    "# Split and scale\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "X_train_s = scaler_X.fit_transform(X_train)\n",
    "X_test_s = scaler_X.transform(X_test)\n",
    "y_train_s = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "y_test_s = scaler_y.transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "print(f\"Dataset: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "print(f\"Train: {len(X_train)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc618dba",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b50fb4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(1, activation='linear')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a47ed5",
   "metadata": {},
   "source": [
    "## Optimizer Comparison Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "004e86a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train with different optimizers\n",
    "def train_model(optimizer_name, optimizer):\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    \n",
    "    if optimizer_name == 'SGD':\n",
    "        # SGD processes one sample at a time\n",
    "        history = model.fit(X_train_s, y_train_s, epochs=50, batch_size=1, verbose=0)\n",
    "    elif optimizer_name == 'Mini-batch':\n",
    "        # Mini-batch with batch size 32\n",
    "        history = model.fit(X_train_s, y_train_s, epochs=50, batch_size=32, verbose=0)\n",
    "    else:  # Batch\n",
    "        # Batch uses entire dataset\n",
    "        history = model.fit(X_train_s, y_train_s, epochs=50, batch_size=len(X_train_s), verbose=0)\n",
    "    \n",
    "    # Predict and calculate MSE\n",
    "    y_pred_s = model.predict(X_test_s, verbose=0)\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_s.reshape(-1, 1)).flatten()\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    return mse, history.history['loss'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a8398c",
   "metadata": {},
   "source": [
    "## Experimental Results and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cad0b321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizer Comparison:\n",
      "Batch GD    : MSE = 5813681828, Final Loss = 0.115373\n",
      "SGD         : MSE = 86287625, Final Loss = 0.001782\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000216109F8C10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Mini-batch  : MSE = 84066890, Final Loss = 0.001697\n"
     ]
    }
   ],
   "source": [
    "# Test different optimizers\n",
    "optimizers = [\n",
    "    ('Batch GD', keras.optimizers.SGD(learning_rate=0.01)),\n",
    "    ('SGD', keras.optimizers.SGD(learning_rate=0.01)),\n",
    "    ('Mini-batch', keras.optimizers.SGD(learning_rate=0.01))\n",
    "]\n",
    "\n",
    "print(\"\\nOptimizer Comparison:\")\n",
    "for name, optimizer in optimizers:\n",
    "    # Reset model weights\n",
    "    model = keras.Sequential([keras.layers.Dense(1, activation='linear')])\n",
    "    mse, final_loss = train_model(name, optimizer)\n",
    "    print(f\"{name:12}: MSE = {mse:.0f}, Final Loss = {final_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7312a1f",
   "metadata": {},
   "source": [
    "Batch GD: Lowest MSE, smoothest convergence, slowest per-epoch training<br>\n",
    "Mini-batch GD: Balanced performance, moderate convergence stability<br>\n",
    "SGD: Higher MSE, noisier convergence, fastest per-update training<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89b24c4",
   "metadata": {},
   "source": [
    "## Home Security Alarm System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dea6446",
   "metadata": {},
   "source": [
    "## Training Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1240eab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "Motion | Door | Alarm\n",
      "--------------------\n",
      "  0   |  0  |   0\n",
      "  0   |  1  |   1\n",
      "  1   |  0  |   1\n",
      "  1   |  1  |   1\n"
     ]
    }
   ],
   "source": [
    "# Training data\n",
    "X_alarm = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float32)\n",
    "y_alarm = np.array([0, 1, 1, 1], dtype=np.float32)\n",
    "\n",
    "print(\"Training Data:\")\n",
    "print(\"Motion | Door | Alarm\")\n",
    "print(\"-\" * 20)\n",
    "for i in range(len(X_alarm)):\n",
    "    print(f\"  {int(X_alarm[i,0])}   |  {int(X_alarm[i,1])}  |   {int(y_alarm[i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba3c519",
   "metadata": {},
   "source": [
    "This represents a logical OR operation - the alarm triggers when at least one condition is met. This is a linearly separable problem, making it perfect for a single-layer perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3677f476",
   "metadata": {},
   "source": [
    "## Training Step and Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "379509fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Custom training loop to show intermediate steps\n",
    "class AlarmNetwork:\n",
    "    def __init__(self):\n",
    "        # Initialize weights manually\n",
    "        self.w = tf.Variable(tf.random.uniform([2, 1], -0.5, 0.5), name='weights')\n",
    "        self.b = tf.Variable(tf.random.uniform([1], -0.5, 0.5), name='bias')\n",
    "        self.lr = 0.1\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = tf.matmul(x, self.w) + self.b\n",
    "        return tf.sigmoid(z)\n",
    "    \n",
    "    def train_step(self, x, y_true):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self.forward(x)\n",
    "            loss = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "        \n",
    "        gradients = tape.gradient(loss, [self.w, self.b])\n",
    "        \n",
    "        # Manual weight update to show process\n",
    "        self.w.assign_sub(self.lr * gradients[0])\n",
    "        self.b.assign_sub(self.lr * gradients[1])\n",
    "        \n",
    "        return y_pred, loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9615705",
   "metadata": {},
   "source": [
    "The network has 2 weights (one for each input) and 1 bias term. Random initialization between -0.5 and 0.5 gives the network a starting point that isn't too extreme in either direction.<br>\n",
    "The sigmoid function is ideal here because it maps any real number to a probability between 0 and 1, making it perfect for binary classification problems like alarm/no-alarm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309390dc",
   "metadata": {},
   "source": [
    "## Single Epoch Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70d196fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Parameters:\n",
      "w1 = 0.1646\n",
      "w2 = -0.0590\n",
      "b = 0.1879\n",
      "\n",
      "Training Process:\n",
      "\n",
      "Epoch 1:\n",
      "Sample | Input | Target | Predicted | Error\n",
      "---------------------------------------------\n",
      "  1    | [0,0] |   0.0    |   0.5468   | -0.5468\n",
      "  2    | [0,1] |   1.0    |   0.5322   | 0.4678\n",
      "  3    | [1,0] |   1.0    |   0.5872   | 0.4128\n",
      "  4    | [1,1] |   1.0    |   0.5728   | 0.4272\n",
      "\n",
      "MSE: 0.2177\n",
      "\n",
      "Final Parameters:\n",
      "w1 = 0.1748\n",
      "w2 = -0.0479\n",
      "b = 0.1972\n"
     ]
    }
   ],
   "source": [
    "# Create network\n",
    "network = AlarmNetwork()\n",
    "\n",
    "print(f\"\\nInitial Parameters:\")\n",
    "print(f\"w1 = {network.w[0,0].numpy():.4f}\")\n",
    "print(f\"w2 = {network.w[1,0].numpy():.4f}\")\n",
    "print(f\"b = {network.b[0].numpy():.4f}\")\n",
    "\n",
    "# Training\n",
    "print(\"\\nTraining Process:\")\n",
    "for epoch in range(1):  # Show 1 epoch for explanation\n",
    "    print(f\"\\nEpoch {epoch + 1}:\")\n",
    "    \n",
    "    y_pred, loss = network.train_step(X_alarm, y_alarm.reshape(-1, 1))\n",
    "    \n",
    "    print(\"Sample | Input | Target | Predicted | Error\")\n",
    "    print(\"-\" * 45)\n",
    "    for i in range(len(X_alarm)):\n",
    "        target = y_alarm[i]\n",
    "        pred = y_pred[i, 0].numpy()\n",
    "        error = target - pred\n",
    "        print(f\"  {i+1}    | [{int(X_alarm[i,0])},{int(X_alarm[i,1])}] |   {target}    |   {pred:.4f}   | {error:.4f}\")\n",
    "    \n",
    "    print(f\"\\nMSE: {loss.numpy():.4f}\")\n",
    "\n",
    "print(f\"\\nFinal Parameters:\")\n",
    "print(f\"w1 = {network.w[0,0].numpy():.4f}\")\n",
    "print(f\"w2 = {network.w[1,0].numpy():.4f}\")\n",
    "print(f\"b = {network.b[0].numpy():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dcee02",
   "metadata": {},
   "source": [
    "Initial Predictions: Random weights produce predictions around 0.5<br>\n",
    "Errors: Large initially, showing need for training<br>\n",
    "Learning Direction: Positive errors increase weights, negative errors decrease them<br>\n",
    "MSE: High initially, decreases as training progresses<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8038f610",
   "metadata": {},
   "source": [
    "## Final Network Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3f90bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Results:\n",
      "Input | Predicted | Actual\n",
      "-------------------------\n",
      "[0,0] |   0.5491   |   0.0\n",
      "[0,1] |   0.5372   |   1.0\n",
      "[1,0] |   0.5919   |   1.0\n",
      "[1,1] |   0.5803   |   1.0\n"
     ]
    }
   ],
   "source": [
    "# Test final network\n",
    "print(\"\\nFinal Test Results:\")\n",
    "final_pred = network.forward(X_alarm)\n",
    "print(\"Input | Predicted | Actual\")\n",
    "print(\"-\" * 25)\n",
    "for i in range(len(X_alarm)):\n",
    "    pred = final_pred[i, 0].numpy()\n",
    "    actual = y_alarm[i]\n",
    "    print(f\"[{int(X_alarm[i,0])},{int(X_alarm[i,1])}] |   {pred:.4f}   |   {actual}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ab77b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Keras model for alarm system\n",
    "alarm_model = keras.Sequential([\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "alarm_model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.1), \n",
    "                   loss='binary_crossentropy', \n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "history = alarm_model.fit(X_alarm, y_alarm, epochs=300, verbose=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "496b6db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000021612001BD0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Keras Model Results:\n",
      "Input | Predicted | Actual\n",
      "-------------------------\n",
      "[0,0] |   0.3550   |   0.0\n",
      "[0,1] |   0.8770   |   1.0\n",
      "[1,0] |   0.8672   |   1.0\n",
      "[1,1] |   0.9883   |   1.0\n",
      "\n",
      "Final Accuracy: 1.0000\n",
      "Final Loss: 0.1813\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "predictions = alarm_model.predict(X_alarm, verbose=0)\n",
    "print(\"Keras Model Results:\")\n",
    "print(\"Input | Predicted | Actual\")\n",
    "print(\"-\" * 25)\n",
    "for i in range(len(X_alarm)):\n",
    "    pred = predictions[i, 0]\n",
    "    actual = y_alarm[i]\n",
    "    print(f\"[{int(X_alarm[i,0])},{int(X_alarm[i,1])}] |   {pred:.4f}   |   {actual}\")\n",
    "\n",
    "print(f\"\\nFinal Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final Loss: {history.history['loss'][-1]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
